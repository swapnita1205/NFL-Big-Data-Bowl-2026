{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52016,"status":"ok","timestamp":1763931146854,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"},"user_tz":300},"id":"wvdoQBXeN-Bm","outputId":"8b93626c-3bf7-4383-bfa4-e0498e6999f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3R-YRvuxS1Sb","executionInfo":{"status":"ok","timestamp":1763931154516,"user_tz":300,"elapsed":7664,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["%%capture\n","!pip install torch-geometric"]},{"cell_type":"markdown","metadata":{"id":"6scEdDyWLlMD"},"source":["# 3.1 Encoder (temporal per player)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ADa5jGj9MOyH","executionInfo":{"status":"ok","timestamp":1763931630645,"user_tz":300,"elapsed":89,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["# ============================================================\n","# üöÄ FULL PIPELINE: From raw tracking ‚Üí temporal embeddings\n","# ============================================================\n","import pandas as pd\n","import numpy as np\n","import glob\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","\n","# ============================================================\n","# 1Ô∏è‚É£ Normalization\n","# ============================================================\n","def normalize_field_direction(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    mask_left = df[\"play_direction\"].str.lower() == \"left\"\n","\n","    # Flip x\n","    for col in [\"x\", \"ball_land_x\"]:\n","        if col in df.columns:\n","            df.loc[mask_left, col] = 120 - df.loc[mask_left, col]\n","\n","    # Flip angles\n","    for ang_col in [\"o\", \"dir\"]:\n","        if ang_col in df.columns:\n","            df.loc[mask_left, ang_col] = (df.loc[mask_left, ang_col] + 180) % 360\n","\n","    # Center y around midline (26.65)\n","    for col in [\"y\", \"ball_land_y\"]:\n","        if col in df.columns:\n","            df[col] = df[col] - 26.65\n","\n","    return df\n","\n","# ============================================================\n","# 2Ô∏è‚É£ Build per-player kinematics & contextual features\n","# ============================================================\n","def build_player_kinematics_features(df_input: pd.DataFrame, K: int = 10) -> pd.DataFrame:\n","    df = df_input.copy()\n","    df[\"abs_yardline_norm\"] = df[\"absolute_yardline_number\"] / 120.0\n","    df = df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n","\n","    # Compute velocity\n","    df[\"vx\"] = df.groupby([\"game_id\",\"play_id\",\"nfl_id\"])[\"x\"].diff().fillna(0)\n","    df[\"vy\"] = df.groupby([\"game_id\",\"play_id\",\"nfl_id\"])[\"y\"].diff().fillna(0)\n","\n","    # Select last K frames per player\n","    lastK = df.groupby([\"game_id\",\"play_id\",\"nfl_id\"], group_keys=False).apply(lambda g: g.tail(K))\n","\n","    # Compute goal features at throw frame (first of last K)\n","    throw_frame = df.groupby([\"game_id\",\"play_id\",\"nfl_id\"], group_keys=False).apply(lambda g: g.tail(K).head(1))\n","    throw_frame[\"dx_land\"] = throw_frame[\"ball_land_x\"] - throw_frame[\"x\"]\n","    throw_frame[\"dy_land\"] = throw_frame[\"ball_land_y\"] - throw_frame[\"y\"]\n","    throw_frame[\"dist_land\"] = np.hypot(throw_frame[\"dx_land\"], throw_frame[\"dy_land\"])\n","    throw_frame[\"az_land\"] = np.degrees(np.arctan2(throw_frame[\"dy_land\"], throw_frame[\"dx_land\"]))\n","    goal_feats = throw_frame[[\"game_id\",\"play_id\",\"nfl_id\",\"dx_land\",\"dy_land\",\"dist_land\",\"az_land\"]]\n","\n","    # Aggregate kinematics\n","    kin_feats = (\n","        lastK.groupby([\"game_id\",\"play_id\",\"nfl_id\"])\n","        .agg({\n","            \"x\":[\"mean\",\"std\"], \"y\":[\"mean\",\"std\"],\n","            \"vx\":[\"mean\",\"std\"], \"vy\":[\"mean\",\"std\"],\n","            \"s\":[\"mean\",\"std\"], \"a\":[\"mean\",\"std\"],\n","            \"dir\":[\"mean\"], \"o\":[\"mean\"], \"frame_id\":[\"max\"]\n","        })\n","        .reset_index()\n","    )\n","    kin_feats.columns = [\"_\".join(col).rstrip(\"_\") for col in kin_feats.columns]\n","    feats = kin_feats.merge(goal_feats, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n","\n","    # Add roster one-hots\n","    roster_cols = [\"player_side\",\"player_role\",\"player_position\"]\n","    roster = df[[\"game_id\",\"play_id\",\"nfl_id\"] + roster_cols].drop_duplicates([\"game_id\",\"play_id\",\"nfl_id\"])\n","    feats = feats.merge(roster, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n","    feats = pd.get_dummies(feats, columns=roster_cols, prefix=roster_cols)\n","\n","    # Game context\n","    context = (\n","        df.groupby([\"game_id\",\"play_id\",\"nfl_id\"], as_index=False)\n","        .agg(abs_yardline_norm=(\"abs_yardline_norm\",\"mean\"), total_frames=(\"frame_id\",\"max\"))\n","    )\n","    feats = feats.merge(context, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n","    feats[\"time_index_norm\"] = feats[\"frame_id_max\"] / feats[\"total_frames\"]\n","    feats.drop(columns=[\"frame_id_max\",\"total_frames\"], inplace=True)\n","    return feats\n","\n","# ============================================================\n","# 4Ô∏è‚É£ Temporal Encoder (GRU / Transformer)\n","# ============================================================\n","class TemporalTransformer(nn.Module):\n","    def __init__(self, in_dim=8, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n","        super().__init__()\n","\n","        self.input_proj = nn.Linear(in_dim, d_model)\n","\n","        self.pos_emb = nn.Parameter(torch.randn(1, 60, d_model))   # K up to 60\n","\n","        enc_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=n_heads,\n","            dim_feedforward=d_model*4,\n","            dropout=dropout,\n","            batch_first=True,\n","            norm_first=True\n","        )\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n","\n","    def forward(self, x):\n","        # x : (P, K, F)\n","        P,K,F = x.shape\n","\n","        x = self.input_proj(x)\n","\n","        # add positional embeddings: truncate or expand\n","        pos = self.pos_emb[:, :K, :]\n","\n","        x = x + pos\n","\n","        out = self.encoder(x)       # (P,K,D)\n","\n","        # take last token\n","        return out[:, -1, :]        # (P,D)"]},{"cell_type":"markdown","metadata":{"id":"-cQfTHBtSFwd"},"source":["# 3.2 Interaction block (multi-agent)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZJGyjVxUSNnV","executionInfo":{"status":"ok","timestamp":1763931167900,"user_tz":300,"elapsed":3650,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.neighbors import NearestNeighbors\n","from tqdm import tqdm\n","\n","def build_interaction_graphs(df_input: pd.DataFrame, K: int = 6):\n","    \"\"\"\n","    Build K-NN interaction graphs per play at the throw frame.\n","    Each graph connects every player to K nearest neighbors\n","    with edge features [dx, dy, dvx, dvy, ally_flag].\n","\n","    Returns\n","    -------\n","    dict[(game_id, play_id)] = {\n","        \"nodes\": pd.DataFrame of player features,\n","        \"edges\": pd.DataFrame of edge features\n","    }\n","    \"\"\"\n","\n","    df = df_input.copy()\n","\n","    # ------------------------------------------------------------------\n","    # üß≠ 1Ô∏è‚É£ Ensure velocity columns exist\n","    # ------------------------------------------------------------------\n","    if \"vx\" not in df.columns or \"vy\" not in df.columns:\n","        df = df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"]).copy()\n","        df[\"vx\"] = df.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"x\"].diff().fillna(0)\n","        df[\"vy\"] = df.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"y\"].diff().fillna(0)\n","\n","    # ------------------------------------------------------------------\n","    # üïê 2Ô∏è‚É£ Extract throw frame (last frame of input for each player)\n","    # ------------------------------------------------------------------\n","    throw_frame = (\n","        df.groupby([\"game_id\", \"play_id\", \"nfl_id\"], group_keys=False)\n","          .apply(lambda g: g.tail(1))\n","          .reset_index(drop=True)\n","    )\n","\n","    # ------------------------------------------------------------------\n","    # üß© 3Ô∏è‚É£ Build graph per play\n","    # ------------------------------------------------------------------\n","    graphs = {}\n","    plays = throw_frame.groupby([\"game_id\", \"play_id\"])\n","\n","    for (gid, pid), play_df in tqdm(plays, desc=\"Building KNN graphs per play\"):\n","\n","        # node features (one per player)\n","        nodes = play_df[\n","            [\"nfl_id\", \"x\", \"y\", \"vx\", \"vy\", \"player_side\", \"player_role\"]\n","        ].reset_index(drop=True)\n","\n","        coords = nodes[[\"x\", \"y\"]].values\n","\n","        if len(nodes) < 2:\n","            continue  # skip incomplete plays\n","\n","        # Fit KNN (K+1 to include self, drop self edge later)\n","        nbrs = NearestNeighbors(\n","            n_neighbors=min(K + 1, len(nodes)),\n","            algorithm=\"ball_tree\"\n","        ).fit(coords)\n","        distances, indices = nbrs.kneighbors(coords)\n","\n","        edge_records = []\n","        for i, nbr_idxs in enumerate(indices):\n","            for j in nbr_idxs[1:]:  # skip self\n","                src = nodes.iloc[i]\n","                dst = nodes.iloc[j]\n","                dx  = dst[\"x\"]  - src[\"x\"]\n","                dy  = dst[\"y\"]  - src[\"y\"]\n","                dvx = dst[\"vx\"] - src[\"vx\"]\n","                dvy = dst[\"vy\"] - src[\"vy\"]\n","\n","                # NEW FEATURES\n","                dist = np.sqrt(dx*dx + dy*dy + 1e-6)        # distance magnitude\n","                dv   = np.sqrt(dvx*dvx + dvy*dvy + 1e-6)    # relative speed magnitude\n","                bearing = np.arctan2(dy, dx)\n","                cos_bear = np.cos(bearing)\n","                sin_bear = np.sin(bearing)\n","\n","                edge_records.append({\n","                    \"src_id\": src[\"nfl_id\"],\n","                    \"dst_id\": dst[\"nfl_id\"],\n","                    \"dx\": dx,\n","                    \"dy\": dy,\n","                    \"dvx\": dvx,\n","                    \"dvy\": dvy,\n","\n","                    # NEW\n","                    \"dist\": dist,\n","                    \"dv\": dv,\n","                    \"cos_bear\": cos_bear,\n","                    \"sin_bear\": sin_bear,\n","\n","                    \"ally_flag\": 1 if src[\"player_side\"] == dst[\"player_side\"] else 0\n","                })\n","\n","\n","        edges = pd.DataFrame(edge_records)\n","        graphs[(gid, pid)] = {\"nodes\": nodes, \"edges\": edges}\n","\n","    return graphs"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"6MMp_B9RSZtV","executionInfo":{"status":"ok","timestamp":1763931647991,"user_tz":300,"elapsed":24,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["class SpatialTransformer(nn.Module):\n","    def __init__(self, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n","        super().__init__()\n","\n","        layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=n_heads,\n","            dim_feedforward=d_model*4,\n","            dropout=dropout,\n","            batch_first=True,\n","            norm_first=True\n","        )\n","        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n","\n","        # project edge_attr ‚Üí attention bias\n","        self.edge_proj = nn.Linear(9, d_model)\n","\n","    def forward(self, h_nodes, edge_index, edge_attr):\n","        \"\"\"\n","        h_nodes    : (P,D)\n","        edge_index : (2,E)\n","        edge_attr  : (E,9)\n","        We convert edges ‚Üí full (P,P,d_model) bias matrix.\n","        \"\"\"\n","\n","        P = h_nodes.size(0)\n","        device = h_nodes.device\n","\n","        # build full pairwise bias matrix\n","        bias = torch.zeros(P, P, h_nodes.size(1), device=device)\n","\n","        src, dst = edge_index\n","        e = self.edge_proj(edge_attr)         # (E,D)\n","        bias[src, dst] = e                    # direct fill-in\n","\n","        # convert to additive attention bias\n","        # flatten into (1,P,P,D)\n","        bias = bias.unsqueeze(0)\n","\n","        # Transformer encoder supports \"src_mask\" but not arbitrary bias.\n","        # So we fold bias into embeddings:\n","        h = h_nodes.unsqueeze(0)              # (1,P,D)\n","        h = h + bias.mean(dim=2)              # aggregate bias per node\n","\n","        h = self.encoder(h)                   # (1,P,D)\n","\n","        return h.squeeze(0)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"2Qvd5wwIPC1K","executionInfo":{"status":"ok","timestamp":1763931648010,"user_tz":300,"elapsed":15,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATv2Conv\n","from sklearn.neighbors import NearestNeighbors\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"rwke3RGHSmjy"},"source":["# 3.3 Role-specific adapters"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yAe5P_WJSbFY","executionInfo":{"status":"ok","timestamp":1763931186965,"user_tz":300,"elapsed":1,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ============================================================\n","# üß© 3.3 Role-Specific Adapter Module\n","# ============================================================\n","\n","class RoleSpecificAdapters(nn.Module):\n","    \"\"\"\n","    Each player_role (e.g., Targeted WR, Coverage DB, Passer, Other)\n","    gets its own small MLP adapter that reshapes the 128-D context\n","    embedding into a role-specialized representation.\n","\n","    Input : h_context  ‚Üí (N_players, embed_dim)\n","            role_ids   ‚Üí (N_players,)  integers 0..N_roles-1\n","    Output: h_adapted  ‚Üí (N_players, embed_dim)\n","    \"\"\"\n","    def __init__(self, embed_dim=128, hidden_dim=128, role_names=None):\n","        super().__init__()\n","        if role_names is None:\n","            role_names = [\"Targeted Receiver\", \"Defensive Coverage\", \"Passer\", \"Other Route Runner\"]\n","        self.role_names = role_names\n","        self.n_roles = len(role_names)\n","\n","        # small MLP adapter per role\n","        self.adapters = nn.ModuleDict({\n","            name: nn.Sequential(\n","                nn.Linear(embed_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, embed_dim),\n","                nn.LayerNorm(embed_dim)\n","            )\n","            for name in role_names\n","        })\n","\n","        # shared fallback (for unseen / undefined roles)\n","        self.default_adapter = nn.Sequential(\n","            nn.Linear(embed_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, embed_dim),\n","            nn.LayerNorm(embed_dim)\n","        )\n","\n","    def forward(self, h_context, role_ids, role_mapping):\n","        \"\"\"\n","        h_context : (N, D)\n","        role_ids  : list/series of textual roles matching role_mapping keys\n","        role_mapping : {role_name : idx}\n","        \"\"\"\n","        outputs = []\n","        for i, r in enumerate(role_ids):\n","            role_name = None\n","            # reverse-map index to string\n","            if isinstance(r, (int, np.integer)):\n","                # find key by index\n","                for k,v in role_mapping.items():\n","                    if v == r:\n","                        role_name = k\n","                        break\n","            else:\n","                role_name = r\n","\n","            if role_name in self.adapters:\n","                out = self.adapters[role_name](h_context[i])\n","            else:\n","                out = self.default_adapter(h_context[i])\n","            outputs.append(out)\n","\n","        return torch.stack(outputs, dim=0)"]},{"cell_type":"markdown","metadata":{"id":"nPPXU07KSJmA"},"source":["# 3.4 Two-stream decoder (direct multi-horizon residuals)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"O6MOLNQtNMrt","executionInfo":{"status":"ok","timestamp":1763931186966,"user_tz":300,"elapsed":1,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# ----------------------------\n","# Helpers: time embeddings\n","# ----------------------------\n","class TimeEmbedding(nn.Module):\n","    \"\"\"\n","    Sinusoidal + learned projection for tau in [0,1].\n","    \"\"\"\n","    def __init__(self, emb_dim=64, n_freq=8):\n","        super().__init__()\n","        self.n_freq = n_freq\n","        self.proj = nn.Linear(2*n_freq, emb_dim)\n","\n","    def forward(self, tau):  # tau: (P, T) in [0,1]\n","        P, T = tau.shape\n","        device = tau.device\n","        # [P, T, 2*n_freq]\n","        freqs = torch.arange(self.n_freq, device=device).float()  # 0..n-1\n","        ang = tau.unsqueeze(-1) * (2.0 * np.pi * (freqs + 1.0))   # avoid 0 freq\n","        sin = torch.sin(ang)\n","        cos = torch.cos(ang)\n","        feats = torch.cat([sin, cos], dim=-1)                     # (P, T, 2*n_freq)\n","        return self.proj(feats)                                   # (P, T, emb_dim)\n","\n","\n","# ----------------------------\n","# Two-stream decoder\n","# ----------------------------\n","class TwoStreamDecoder(nn.Module):\n","    \"\"\"\n","    Stream A (Goal-drift): drives motion toward (ball_land_x, ball_land_y).\n","    Stream B (Interaction correction): local evasive/collision adjustments from context.\n","\n","    Inputs per play:\n","      h_role: (P, D)     role-specific embeddings from 3.3\n","      goal_feat: (P, G)  per-player goal features [dx0, dy0, dist0, ux, uy]\n","      tau_seq: (P, T)    time-to-land values in [0,1] per player\n","      horizon: (P,) long per-player num_frames_output (1..N_max)\n","    Outputs:\n","      dxy: (P, T, 2)     residuals Œîx,Œîy relative to last input frame\n","      mask: (P, T)       1 within horizon, 0 after\n","    \"\"\"\n","    def __init__(self, d_model=128, time_dim=64, goal_dim=5, hidden=256, N_max=30):\n","        super().__init__()\n","        self.goal_dim = goal_dim\n","        self.N_max = N_max\n","        self.time_emb = TimeEmbedding(emb_dim=time_dim, n_freq=8)\n","\n","        inA = d_model + time_dim + goal_dim\n","        inB = d_model + time_dim\n","\n","        # Stream A: smooth drift toward goal\n","        self.streamA = nn.Sequential(\n","            nn.Linear(inA, hidden),\n","            nn.ReLU(),\n","            nn.Linear(hidden, hidden),\n","            nn.ReLU(),\n","            nn.Linear(hidden, 2)  # Œîx, Œîy per step\n","        )\n","\n","        # Stream B: local interaction correction\n","        self.streamB = nn.Sequential(\n","            nn.Linear(inB, hidden),\n","            nn.ReLU(),\n","            nn.Linear(hidden, hidden),\n","            nn.ReLU(),\n","            nn.Linear(hidden, 2)\n","        )\n","\n","    def forward(self, h_role, goal_feat, tau_seq, horizon):\n","        \"\"\"\n","        h_role:   (P, D)\n","        goal_feat:(P, 5)  = [dx0, dy0, dist0, ux, uy]\n","        tau_seq:  (P, T)\n","        horizon:  (P,)\n","        \"\"\"\n","        P, D = h_role.shape\n","        T = tau_seq.shape[1]\n","\n","        # ‚úÖ Allow variable horizon lengths (for curriculum training)\n","        if T != self.N_max:\n","            # ensure consistent device, dtype, and leading dimension P\n","            pad_len = max(self.N_max - T, 0)\n","            if pad_len > 0:\n","                pad = torch.ones(\n","                    (P, pad_len),\n","                    dtype=tau_seq.dtype,\n","                    device=tau_seq.device\n","                )\n","                tau_seq = torch.cat([tau_seq, pad], dim=1)\n","            elif T > self.N_max:\n","                tau_seq = tau_seq[:, :self.N_max]\n","            T = self.N_max\n","\n","\n","        # time embedding\n","        t_emb = self.time_emb(tau_seq)             # (P, T, time_dim)\n","\n","        # expand static inputs across time\n","        h_rep = h_role.unsqueeze(1).expand(P, T, D)        # (P, T, D)\n","        g_rep = goal_feat.unsqueeze(1).expand(P, T, goal_feat.size(1))  # (P, T, 5)\n","\n","        # stream A: goal drift\n","        a_in = torch.cat([h_rep, t_emb, g_rep], dim=-1)    # (P, T, D+time_dim+5)\n","        dA = self.streamA(a_in)                            # (P, T, 2)\n","\n","        # stream B: interaction correction\n","        b_in = torch.cat([h_rep, t_emb], dim=-1)           # (P, T, D+time_dim)\n","        dB = self.streamB(b_in)                            # (P, T, 2)\n","\n","        dxy = dA + dB                                      # (P, T, 2)\n","\n","        # horizon mask: 1..H_i active\n","        device = h_role.device\n","        t_idx = torch.arange(T, device=device).unsqueeze(0).expand(P, T)  # 0..T-1\n","        mask = (t_idx < horizon.unsqueeze(1)).float()                     # (P, T)\n","\n","        return dxy, mask\n","\n","\n","# ----------------------------\n","# Per-play tensor builder\n","# ----------------------------\n","def prepare_play_decoder_inputs(play_graph_nodes: pd.DataFrame,\n","                                df_in_norm: pd.DataFrame,\n","                                game_id: int, play_id: int,\n","                                N_max: int = 30,\n","                                device: str = \"cpu\"):\n","    \"\"\"\n","    Builds inputs for TwoStreamDecoder from one play.\n","\n","    Returns:\n","      x0y0:      (P, 2) last input frame positions (for later reconstruction)\n","      h0_goal:   (P, 5) [dx0, dy0, dist0, ux, uy]\n","      tau_seq:   (P, N_max) tau = t / num_frames_output (clipped to 1)\n","      horizon:   (P,)  long\n","      (Plus convenience dict with ball_land per play)\n","    \"\"\"\n","    nodes = play_graph_nodes.reset_index(drop=True).copy()  # needs columns: nfl_id,x,y,player_role,etc.\n","\n","    # last input position per player is already in nodes['x','y'] from 1.3 throw-frame snapshot\n","    x0y0 = torch.tensor(nodes[[\"x\", \"y\"]].to_numpy(), dtype=torch.float32, device=device)  # (P,2)\n","\n","    # ball landing (per play) from df_in_norm (any row of this play has same ball_land)\n","    play_rows = df_in_norm[(df_in_norm.game_id == game_id) & (df_in_norm.play_id == play_id)]\n","    bx = float(play_rows[\"ball_land_x\"].iloc[-1])\n","    by = float(play_rows[\"ball_land_y\"].iloc[-1])\n","\n","    # goal vector at throw\n","    dx0 = torch.tensor((bx - nodes[\"x\"]).to_numpy(), dtype=torch.float32, device=device)\n","    dy0 = torch.tensor((by - nodes[\"y\"]).to_numpy(), dtype=torch.float32, device=device)\n","    dist0 = torch.sqrt(dx0**2 + dy0**2) + 1e-6\n","    ux = dx0 / dist0\n","    uy = dy0 / dist0\n","    h0_goal = torch.stack([dx0, dy0, dist0, ux, uy], dim=-1)  # (P,5)\n","\n","    # per-player horizon from input table\n","    # num_frames_output is per (game,play,nfl). Take the last input row per player.\n","    horizon_np = (\n","        play_rows.sort_values([\"nfl_id\",\"frame_id\"])\n","                 .groupby(\"nfl_id\")[\"num_frames_output\"]\n","                 .last()\n","                 .reindex(nodes[\"nfl_id\"])\n","                 .fillna(0).to_numpy(dtype=np.int64)\n","    )\n","    horizon = torch.tensor(np.minimum(horizon_np, N_max), dtype=torch.long, device=device)  # (P,)\n","\n","    # tau sequence per player (P, T)\n","    T = N_max\n","    t_grid = torch.arange(1, T+1, device=device).float().unsqueeze(0).expand(len(nodes), T)  # 1..T\n","    denom = torch.clamp(horizon.unsqueeze(1).float(), min=1.0)\n","    tau_seq = torch.clamp(t_grid / denom, max=1.0)  # (P,T) in [0,1]\n","\n","    meta = dict(ball_land=(bx, by))\n","    return x0y0, h0_goal, tau_seq, horizon, meta\n","\n","\n","# ----------------------------\n","# Example usage for one play\n","# ----------------------------\n","# Given from earlier steps:\n","# - graphs : dict[(gid,pid)] ‚Üí {\"nodes\": df_nodes, \"edges\": df_edges}\n","# - embeds after 3.2 & 3.3: h_role for this play (torch, shape (P, 128))\n","# - df_in_norm: normalized inputs for the week\n","\n","def run_decoder_for_one_play(graphs, df_in_norm, h_role_dict_for_play,\n","                             game_id, play_id, N_max=30, device=\"cpu\"):\n","    \"\"\"\n","    h_role_dict_for_play: map nfl_id -> 128-d torch tensor for that play,\n","                          or a stacked tensor aligned with graphs[(gid,pid)][\"nodes\"]\n","    \"\"\"\n","    play_nodes = graphs[(game_id, play_id)][\"nodes\"]  # DataFrame with nfl_id,x,y,...\n","\n","    # Ensure h_role is aligned with node order\n","    if isinstance(h_role_dict_for_play, torch.Tensor):\n","        h_role = h_role_dict_for_play.to(device)  # assume already aligned (P,128)\n","    else:\n","        rows = []\n","        for nid in play_nodes[\"nfl_id\"].tolist():\n","            rows.append(h_role_dict_for_play[nid].unsqueeze(0))\n","        h_role = torch.cat(rows, dim=0).to(device)  # (P,128)\n","\n","    x0y0, goal_feat, tau_seq, horizon, meta = prepare_play_decoder_inputs(\n","        play_nodes, df_in_norm, game_id, play_id, N_max=N_max, device=device\n","    )\n","\n","    decoder = TwoStreamDecoder(d_model=h_role.size(1), time_dim=64, goal_dim=5, hidden=256, N_max=N_max).to(device)\n","    dxy, mask = decoder(h_role, goal_feat, tau_seq, horizon)   # (P,T,2), (P,T)\n","\n","    # reconstruct absolute positions if desired\n","    # cumulative residuals from last input pos\n","    cum_dxy = torch.cumsum(dxy, dim=1)                         # (P,T,2)\n","    xy_pred = x0y0.unsqueeze(1) + cum_dxy                      # (P,T,2)\n","    # apply mask (zero out invalid timesteps)\n","    xy_pred = xy_pred * mask.unsqueeze(-1)\n","\n","    return dict(\n","        dxy=dxy, mask=mask, xy_pred=xy_pred, x0y0=x0y0, goal_feat=goal_feat,\n","        tau_seq=tau_seq, horizon=horizon, ball_land=meta[\"ball_land\"]\n","    )"]},{"cell_type":"markdown","metadata":{"id":"xVh8ub0rUFMP"},"source":["# 3.5 Huber Loss"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"QL7y2alxSU53","executionInfo":{"status":"ok","timestamp":1763931186968,"user_tz":300,"elapsed":1,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["class TemporalHuber(nn.Module):\n","    def __init__(self, delta=0.5, time_decay=0.03):\n","        super().__init__()\n","        self.delta = delta\n","        self.time_decay = time_decay\n","\n","    def forward(self, pred, target, mask):\n","        err = pred - target\n","        abs_err = torch.abs(err)\n","\n","        huber = torch.where(\n","            abs_err <= self.delta,\n","            0.5 * err * err,\n","            self.delta * (abs_err - 0.5 * self.delta)\n","        )\n","\n","        if self.time_decay > 0:\n","            L = pred.size(1)\n","            t = torch.arange(L, device=pred.device).float()\n","            weight = torch.exp(-self.time_decay * t).view(1, L, 1)\n","            huber = huber * weight\n","            mask = mask.unsqueeze(-1) * weight\n","\n","        return (huber * mask).sum() / (mask.sum() + 1e-8)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"H6QqWOWuV1w7","executionInfo":{"status":"ok","timestamp":1763931186972,"user_tz":300,"elapsed":3,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"outputs":[],"source":["def normalize_output_like_input(df_out: pd.DataFrame, df_in: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Flip output x for left plays and center y, using play_direction from input.\"\"\"\n","    df_out = df_out.copy()\n","    dir_map = (df_in[[\"game_id\",\"play_id\",\"play_direction\"]]\n","               .drop_duplicates()\n","               .assign(is_left=lambda d: d[\"play_direction\"].str.lower()==\"left\")\n","               .drop(columns=\"play_direction\"))\n","    df_out = df_out.merge(dir_map, on=[\"game_id\",\"play_id\"], how=\"left\")\n","    df_out.loc[df_out[\"is_left\"]==True, \"x\"] = 120 - df_out.loc[df_out[\"is_left\"]==True, \"x\"]\n","    df_out[\"y\"] = df_out[\"y\"] - 26.65\n","    return df_out.drop(columns=[\"is_left\"])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131624,"status":"ok","timestamp":1763931318597,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"},"user_tz":300},"id":"sBkuquB149YR","outputId":"e4ea2640-3f0e-44bc-e6af-8324eda06b7f","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["üìÑ Loading week w01 ...\n","üìÑ Loading week w02 ...\n","üìÑ Loading week w03 ...\n","üìÑ Loading week w04 ...\n","üìÑ Loading week w05 ...\n","üìÑ Loading week w06 ...\n","üìÑ Loading week w07 ...\n","üìÑ Loading week w08 ...\n","üìÑ Loading week w09 ...\n","üìÑ Loading week w10 ...\n","üìÑ Loading week w11 ...\n","üìÑ Loading week w12 ...\n","üìÑ Loading week w13 ...\n","üìÑ Loading week w14 ...\n","üìÑ Loading week w15 ...\n","üìÑ Loading week w16 ...\n","üìÑ Loading week w17 ...\n","üìÑ Loading week w18 ...\n","‚úÖ Combined input shape:  (4880579, 23)\n","‚úÖ Combined output shape: (562936, 6)\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2984543795.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: g.tail(1))\n","Building KNN graphs per play: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14108/14108 [01:05<00:00, 213.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Graphs built successfully: 12966\n"]}],"source":["import pandas as pd\n","\n","data_path = \"/content/drive/MyDrive/NFL Big Data Bowl 2026/nfl-big-data-bowl-2026-prediction/train\"\n","weeks = [\"w01\", \"w02\" , \"w03\", \"w04\", \"w05\", \"w06\" , \"w07\", \"w08\", \"w09\", \"w10\", \"w11\" , \"w12\", \"w13\", \"w14\", \"w15\", \"w16\", \"w17\", \"w18\"]\n","\n","df_in_list, df_out_list = [], []\n","\n","for w in weeks:\n","    f_in = f\"{data_path}/input_2023_{w}.csv\"\n","    f_out = f\"{data_path}/output_2023_{w}.csv\"\n","    print(f\"üìÑ Loading week {w} ...\")\n","    df_in_list.append(pd.read_csv(f_in))\n","    df_out_list.append(pd.read_csv(f_out))\n","\n","df_in_raw  = pd.concat(df_in_list, ignore_index=True)\n","df_out_raw = pd.concat(df_out_list, ignore_index=True)\n","\n","print(f\"‚úÖ Combined input shape:  {df_in_raw.shape}\")\n","print(f\"‚úÖ Combined output shape: {df_out_raw.shape}\")\n","\n","keys = [\"game_id\",\"play_id\",\"nfl_id\"]\n","common = (\n","    df_out_raw[keys].drop_duplicates()\n","    .merge(df_in_raw[keys].drop_duplicates(), on=keys, how=\"inner\")\n",")\n","df_in_raw  = df_in_raw.merge(common, on=keys, how=\"inner\")\n","df_out_raw = df_out_raw.merge(common, on=keys, how=\"inner\")\n","\n","df_in_norm  = normalize_field_direction(df_in_raw)\n","df_out_norm = normalize_output_like_input(df_out_raw, df_in_raw)\n","\n","df_in_norm = df_in_norm.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n","df_in_norm[\"vx\"] = df_in_norm.groupby([\"game_id\",\"play_id\",\"nfl_id\"])[\"x\"].diff().fillna(0)\n","df_in_norm[\"vy\"] = df_in_norm.groupby([\"game_id\",\"play_id\",\"nfl_id\"])[\"y\"].diff().fillna(0)\n","\n","graphs = build_interaction_graphs(df_in_norm, K=6)\n","# /content/drive/MyDrive/NFL Big Data Bowl 2026/nfl-big-data-bowl-2026-prediction/train/input_2023_w01.csv\n","print(\"‚úÖ Graphs built successfully:\", len(graphs))"]},{"cell_type":"markdown","metadata":{"id":"mvrbH5gDLIyk"},"source":["# 4 - Training recipe (stable, within budget)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Ey4dmttp7BPf","executionInfo":{"status":"ok","timestamp":1763931559505,"user_tz":300,"elapsed":240893,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"10233c47-37e6-4739-966c-e9c000614fe1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2365959681.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  edge_index = torch.tensor([src_idx, dst_idx], dtype=torch.long)\n"]}],"source":["def precompute_graph_tensors(graphs, df_out_norm, df_in_norm, N_max=30):\n","    graphs_fast = {}\n","\n","    for (gid, pid), g in graphs.items():\n","\n","        nodes = g[\"nodes\"].sort_values(\"nfl_id\").reset_index(drop=True)\n","        edges = g[\"edges\"]\n","\n","        player_ids = nodes[\"nfl_id\"].tolist()\n","        P = len(player_ids)\n","\n","        # -------- node xy --------\n","        node_xy = torch.tensor(nodes[[\"x\",\"y\"]].values, dtype=torch.float32)\n","\n","        # -------- roles ----------\n","        roles = nodes[\"player_role\"].tolist()\n","\n","        # -------- edges ----------\n","        id_new = {nid:i for i,nid in enumerate(player_ids)}\n","        edges = edges[edges[\"src_id\"].isin(id_new) & edges[\"dst_id\"].isin(id_new)]\n","\n","        src_idx = edges[\"src_id\"].map(id_new).to_numpy()\n","        dst_idx = edges[\"dst_id\"].map(id_new).to_numpy()\n","\n","        edge_index = torch.tensor([src_idx, dst_idx], dtype=torch.long)\n","        edge_attr  = torch.tensor(\n","            edges[[\"dx\",\"dy\",\"dvx\",\"dvy\",\"dist\",\"dv\",\"cos_bear\",\"sin_bear\",\"ally_flag\"]]\n","            .to_numpy(np.float32)\n","        )\n","\n","        # -------- global context --------\n","        global_ctx = torch.tensor([\n","            nodes[\"x\"].mean(),\n","            nodes[\"y\"].mean(),\n","            0.42\n","        ], dtype=torch.float32)\n","\n","        # -------- PRECOMPUTE XY_TRUE --------\n","        df_out = df_out_norm[(df_out_norm.game_id==gid)&(df_out_norm.play_id==pid)]\n","        df_out = df_out.sort_values([\"nfl_id\",\"frame_id\"])\n","        T = N_max\n","        xy_true = np.zeros((P, N_max, 2), dtype=np.float32)\n","\n","        for i,nid in enumerate(player_ids):\n","            # extract all future frames\n","            sub = df_out[df_out[\"nfl_id\"]==nid][[\"x\",\"y\"]].to_numpy(np.float32)\n","\n","            # pad or trim to N_max\n","            if sub.shape[0] < N_max:\n","                pad = np.repeat(sub[-1:], N_max - sub.shape[0], axis=0)\n","                sub = np.vstack([sub, pad])\n","            else:\n","                sub = sub[:N_max]\n","\n","            xy_true[i] = sub\n","\n","        xy_true = torch.tensor(xy_true, dtype=torch.float32)\n","\n","        # -------- PRECOMPUTE DECODER INPUTS (goal_feat, tau_seq, horizon) --------\n","        x0y0, goal_feat, tau_seq, horizon, meta = prepare_play_decoder_inputs(\n","            nodes, df_in_norm, gid, pid, N_max=N_max, device=\"cpu\"\n","        )\n","        # x0y0 should match node_xy; we keep node_xy as-is\n","\n","        graphs_fast[(gid,pid)] = {\n","            \"player_ids\": player_ids,\n","            \"node_xy\": node_xy,              # (P,2)\n","            \"roles\": roles,                  # list len P\n","            \"edge_index\": edge_index,        # (2,E)\n","            \"edge_attr\": edge_attr,          # (E,9)\n","            \"global_ctx\": global_ctx,        # (3,)\n","            \"xy_true\": xy_true,              # (P,T,2)\n","            \"goal_feat\": goal_feat,          # (P,5)\n","            \"tau_seq\": tau_seq,              # (P,N_max)\n","            \"horizon\": horizon,              # (P,)\n","        }\n","\n","    return graphs_fast\n","\n","graphs_fast = precompute_graph_tensors(graphs, df_out_norm, df_in_norm, N_max=30)\n"]},{"cell_type":"code","source":["# =========================\n","# üîß Utilities / Seeding\n","# =========================\n","import os, random, math\n","import numpy as np\n","import torch\n","import time\n","from torch import nn\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n","from contextlib import nullcontext\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def set_seed(seed: int):\n","    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# =========================================================\n","# üì¶ Dataset that batches plays (variable players/frames)\n","# =========================================================\n","class PlayDataset(Dataset):\n","    def __init__(self, df_in_norm, df_out_norm, graphs, K=10, features=(\"x\",\"y\",\"vx\",\"vy\",\"s\",\"a\",\"dir\",\"o\")):\n","        self.df_in = df_in_norm\n","        self.df_out = df_out_norm\n","        self.graphs = graphs\n","        self.K = K\n","        self.features = list(features)\n","        # index as list of (gid, pid)\n","        self.index = list(graphs.keys())\n","\n","    def __len__(self):\n","        return len(self.index)\n","\n","    def _build_hist_with_temporal_dropout(self, gid, pid, player_ids, drop_last=0):\n","        g = self.df_in[(self.df_in.game_id==gid) & (self.df_in.play_id==pid)]\n","\n","        P = len(player_ids)\n","        F = len(self.features)\n","        K = self.K\n","\n","        x_hist = torch.zeros((P, K, F), dtype=torch.float32)\n","\n","        for pi, nid in enumerate(player_ids):\n","            gp = g[g[\"nfl_id\"] == nid].sort_values(\"frame_id\").tail(K)\n","            arr = gp[self.features].to_numpy(np.float32)\n","\n","            if arr.shape[0] < K:\n","                pad = np.zeros((K - arr.shape[0], F), dtype=np.float32)\n","                arr = np.vstack([pad, arr])\n","\n","            if drop_last > 0:\n","                arr[-drop_last:] = 0.0\n","\n","            x_hist[pi] = torch.from_numpy(arr)\n","\n","        return x_hist\n","\n","    def __getitem__(self, idx):\n","        gid, pid = self.index[idx]\n","        drop_last = np.random.randint(0, 4)\n","        graph = self.graphs[(gid, pid)]\n","        player_ids = graph[\"player_ids\"]\n","        x_hist = self._build_hist_with_temporal_dropout(gid, pid, player_ids, drop_last)\n","\n","        return {\n","            \"gid\": gid,\n","            \"pid\": pid,\n","            \"x_hist\": x_hist,         # (P,K,F)\n","            \"player_ids\": player_ids, # list length P\n","            \"graph\": graph            # already precomputed graphs_fast entry\n","        }\n","\n","\n","def play_collate(batch):\n","    \"\"\"\n","    batch: list of length B, each item is a dict from __getitem__.\n","    We pack all players across all plays into one big batch dimension P_tot.\n","    \"\"\"\n","    # For clarity\n","    x_hist_list = []\n","    node_xy_list = []\n","    edge_index_list = []\n","    edge_attr_list = []\n","    goal_feat_list = []\n","    tau_seq_list = []\n","    horizon_list = []\n","    xy_true_list = []\n","    roles_all = []\n","    global_ctx_list = []\n","    play_ids = []  # (gid,pid) per play\n","    play_ptr = []  # boundaries in node dimension\n","\n","    offset = 0\n","    for item in batch:\n","        g = item[\"graph\"]\n","        P_i = len(g[\"player_ids\"])\n","\n","        # history\n","        x_hist_list.append(item[\"x_hist\"])              # (P_i,K,F)\n","\n","        # graph stuff\n","        node_xy_list.append(g[\"node_xy\"])               # (P_i,2)\n","        edge_index_i = g[\"edge_index\"] + offset         # (2,E_i) reindexed\n","        edge_index_list.append(edge_index_i)\n","        edge_attr_list.append(g[\"edge_attr\"])           # (E_i,9)\n","\n","        # decoder inputs\n","        goal_feat_list.append(g[\"goal_feat\"])           # (P_i,5)\n","        tau_seq_list.append(g[\"tau_seq\"])               # (P_i,N_max)\n","        horizon_list.append(g[\"horizon\"])               # (P_i,)\n","\n","        # targets\n","        xy_true_list.append(g[\"xy_true\"])               # (P_i,T,2)\n","\n","        # roles and global ctx\n","        roles_all.extend(g[\"roles\"])                    # flat list len P_tot\n","        global_ctx_list.append(g[\"global_ctx\"])         # (3,)\n","\n","        play_ids.append((item[\"gid\"], item[\"pid\"]))\n","        play_ptr.append(offset)\n","\n","        offset += P_i\n","\n","    # concat everything\n","    x_hist   = torch.cat(x_hist_list, dim=0)            # (P_tot,K,F)\n","    node_xy  = torch.cat(node_xy_list, dim=0)           # (P_tot,2)\n","    edge_index = torch.cat(edge_index_list, dim=1)      # (2,E_tot)\n","    edge_attr  = torch.cat(edge_attr_list, dim=0)       # (E_tot,9)\n","    goal_feat  = torch.cat(goal_feat_list, dim=0)       # (P_tot,5)\n","    tau_seq    = torch.cat(tau_seq_list, dim=0)         # (P_tot,N_max)\n","    horizon    = torch.cat(horizon_list, dim=0)         # (P_tot,)\n","    xy_true    = torch.cat(xy_true_list, dim=0)         # (P_tot,T,2)\n","    global_ctx_batch = torch.stack(global_ctx_list, dim=0)  # (B,3)\n","    # simple aggregate: mean ctx across plays (or you could ignore)\n","    global_ctx = global_ctx_batch.mean(dim=0)           # (3,)\n","\n","    return {\n","        \"x_hist\": x_hist,                   # (P_tot,K,F)\n","        \"node_xy\": node_xy,                 # (P_tot,2)\n","        \"edge_index\": edge_index,           # (2,E_tot)\n","        \"edge_attr\": edge_attr,             # (E_tot,9)\n","        \"goal_feat\": goal_feat,             # (P_tot,5)\n","        \"tau_seq\": tau_seq,                 # (P_tot,N_max)\n","        \"horizon\": horizon,                 # (P_tot,)\n","        \"xy_true\": xy_true,                 # (P_tot,T,2)\n","        \"roles\": roles_all,                 # list len P_tot\n","        \"global_ctx\": global_ctx,           # (3,)\n","        \"play_ids\": play_ids,               # list of (gid,pid)\n","        \"play_ptr\": play_ptr,               # list of starting indices per play\n","    }\n","\n","# =========================================================\n","# üß† End-to-end model wrapper (enc + GAT + role adapters + dec)\n","# =========================================================\n","class End2EndModel(nn.Module):\n","    def __init__(self, in_dim=8, embed_dim=128, d_model=128, time_dim=64, goal_dim=5,\n","                 hidden=256, N_max=30, gat_edge_dim=9, gat_heads=4, gat_layers=2,\n","                 role_names=(\"Targeted Receiver\", \"Defensive Coverage\", \"Passer\", \"Other Route Runner\")):\n","        super().__init__()\n","        # Keep param count ~3‚Äì5M by modest dims\n","        self.encoder = TemporalTransformer(in_dim=in_dim, d_model=embed_dim)\n","        self.spatial = SpatialTransformer(d_model=embed_dim)\n","        self.adapters = RoleSpecificAdapters(embed_dim=embed_dim, hidden_dim=embed_dim,\n","                                             role_names=list(role_names))\n","        self.decoder = TwoStreamDecoder(d_model=embed_dim, time_dim=time_dim,\n","                                        goal_dim=goal_dim, hidden=hidden, N_max=N_max)\n","        self.role_names = list(role_names)\n","        self.role_map = {r:i for i,r in enumerate(self.role_names)}\n","        self.N_max = N_max\n","\n","    def forward_batch(self, batch, N_max_curr):\n","        \"\"\"\n","        batch: dict from play_collate\n","        Processes all players from all plays in one shot.\n","        \"\"\"\n","        # unpack & move to device\n","        x_hist   = batch[\"x_hist\"].to(DEVICE).float()          # (P_tot,K,F)\n","        node_xy  = batch[\"node_xy\"].to(DEVICE).float()         # (P_tot,2)\n","        edge_index = batch[\"edge_index\"].to(DEVICE).long()     # (2,E_tot)\n","        edge_attr  = batch[\"edge_attr\"].to(DEVICE).float()     # (E_tot,9)\n","        goal_feat  = batch[\"goal_feat\"].to(DEVICE).float()     # (P_tot,5)\n","        tau_seq    = batch[\"tau_seq\"][:, :N_max_curr].to(DEVICE).float()  # (P_tot,T)\n","        horizon    = torch.clamp(batch[\"horizon\"], max=N_max_curr).to(DEVICE).long()  # (P_tot,)\n","        global_ctx = batch[\"global_ctx\"].to(DEVICE).float()    # (3,)\n","        roles      = batch[\"roles\"]                            # list len P_tot\n","\n","        h_nodes = self.encoder(x_hist)            # (P,D)\n","        h_ctx = self.spatial(h_nodes, edge_index, edge_attr)\n","\n","        # role adapters\n","        h_role = self.adapters(h_ctx, roles, self.role_map)    # (P_tot,D)\n","\n","        # decoder\n","        dxy, mask = self.decoder(h_role, goal_feat, tau_seq, horizon)  # (P_tot,T,2),(P_tot,T)\n","\n","        xy_pred = node_xy.unsqueeze(1) + torch.cumsum(dxy, dim=1)\n","        xy_pred = xy_pred * mask.unsqueeze(-1)\n","\n","        return xy_pred, mask\n","\n","# =========================================================\n","# üöÄ Full-data training version (no validation split)\n","# =========================================================\n","\n","def train_loop_full_data(df_in_norm, df_out_norm, graphs_fast,\n","                         K=10, N_max=30, batch_size=12, num_workers=0,\n","                         lr=2e-4, wd=1e-4,\n","                         warmup_epochs=2, total_epochs=70,\n","                         pretrain_epochs=20):\n","\n","    # ====================================================\n","    # üì¶ Dataset: use ALL plays for training\n","    # ====================================================\n","    ds_full = PlayDataset(df_in_norm, df_out_norm, graphs_fast, K=K)\n","    dl_train = DataLoader(ds_full, batch_size=batch_size, shuffle=True,\n","                          num_workers=num_workers, collate_fn=play_collate, drop_last=False)\n","\n","    print(f\"üìä Dataset: {len(ds_full)} plays for training (no validation split)\")\n","\n","    # ====================================================\n","    # ‚öôÔ∏è Model + optimizer + schedulers\n","    # ====================================================\n","    model = End2EndModel(in_dim=8, embed_dim=128, d_model=128,\n","                         time_dim=64, goal_dim=5, hidden=256,\n","                         N_max=N_max).to(DEVICE)\n","\n","    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n","    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","\n","    cos = CosineAnnealingLR(optimizer, T_max=total_epochs - warmup_epochs, eta_min=0.0)\n","    warm = LinearLR(optimizer, start_factor=1e-3, end_factor=1.0, total_iters=warmup_epochs)\n","    scheduler = SequentialLR(optimizer, schedulers=[warm, cos], milestones=[warmup_epochs])\n","\n","    scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n","    loss_history = []\n","    # ====================================================\n","    # üèãÔ∏è Training loop (full data)\n","    # ====================================================\n","    for epoch in range(1, total_epochs + 1):\n","        start_time = time.time()\n","        model.train()\n","        N_curr = 10 if epoch <= pretrain_epochs else N_max\n","        epoch_loss, n_plays = 0.0, 0\n","\n","        # timing accumulators\n","        t_forward = 0.0\n","        t_build_xy = 0.0\n","        t_loss = 0.0\n","        t_backward = 0.0\n","        t_batch = 0.0\n","\n","        amp_ctx = torch.amp.autocast('cuda') if DEVICE == \"cuda\" else nullcontext()\n","        with amp_ctx:\n","            for batch in dl_train:\n","                batch_start = time.time()\n","                optimizer.zero_grad(set_to_none=True)\n","\n","                # 1) Forward (batched)\n","                t0 = time.time()\n","                xy_pred, mask = model.forward_batch(batch, N_max_curr=N_curr)\n","                t_forward += time.time() - t0\n","\n","                # 2) Build batched xy_true\n","                t1 = time.time()\n","                xy_true = batch[\"xy_true\"][:, :N_curr].to(DEVICE)   # (P_tot,T,2)\n","                t_build_xy += time.time() - t1\n","\n","                # 3) Loss\n","                t2 = time.time()\n","                T_match = min(xy_pred.shape[1], xy_true.shape[1])\n","                xy_pred_ = xy_pred[:, :T_match, :]\n","                xy_true_ = xy_true[:, :T_match, :]\n","                mask_    = mask[:, :T_match]\n","\n","                loss_val = criterion(xy_pred_, xy_true_, mask_)\n","                n_plays += len(batch[\"play_ids\"])   # count plays\n","                t_loss += time.time() - t2\n","\n","                # 4) Backward + optimizer\n","                t3 = time.time()\n","                scaler.scale(loss_val).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","                t_backward += time.time() - t3\n","\n","                t_batch += time.time() - batch_start\n","                epoch_loss += float(loss_val.detach())\n","\n","        scheduler.step()\n","        train_loss = epoch_loss / max(1, n_plays)\n","        loss_history.append(train_loss)\n","\n","        print(f\"Epoch {epoch:03d} | N_max={N_curr:02d} | LR={scheduler.get_last_lr()[0]:.6f} | \"\n","              f\"TrainLoss={train_loss:.4f}\")\n","        print(f\"Epoch {epoch:03d} completed in {time.time() - start_time:.2f} s\")\n","        print(f\"  forward_one_play:   {t_forward:.2f} s\")\n","        print(f\"  build_xy_true:      {t_build_xy:.2f} s\")\n","        print(f\"  loss compute:        {t_loss:.2f} s\")\n","        print(f\"  backward+optim:      {t_backward:.2f} s\")\n","        print(f\"  batch loop total:    {t_batch:.2f} s\\n\")\n","\n","\n","    return model, loss_history\n","\n","for seed in [3, 4, 5]:\n","    set_seed(seed)\n","    model, _ = train_loop_full_data(\n","        df_in_norm, df_out_norm, graphs_fast,\n","        K=10, N_max=30, batch_size=16,\n","        lr=1e-4, wd=1e-4,\n","        warmup_epochs=3, total_epochs=40, pretrain_epochs=3\n","    )\n","    torch.save(model.state_dict(), f\"/content/final_model_seed{seed}.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vHhgFEPDFO_V","outputId":"4f0186b2-e831-4db3-a53c-fe4fcbf974b1","executionInfo":{"status":"error","timestamp":1763942181606,"user_tz":300,"elapsed":10346091,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["üìä Dataset: 12966 plays for training (no validation split)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 001 | N_max=10 | LR=0.000033 | TrainLoss=0.7815\n","Epoch 001 completed in 263.91 s\n","  forward_one_play:   47.93 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.04 s\n","  backward+optim:      68.69 s\n","  batch loop total:    117.97 s\n","\n","Epoch 002 | N_max=10 | LR=0.000067 | TrainLoss=0.2483\n","Epoch 002 completed in 263.84 s\n","  forward_one_play:   47.71 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.04 s\n","  backward+optim:      69.14 s\n","  batch loop total:    118.21 s\n","\n","Epoch 003 | N_max=10 | LR=0.000100 | TrainLoss=0.1212\n","Epoch 003 completed in 259.77 s\n","  forward_one_play:   47.16 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.04 s\n","  backward+optim:      68.21 s\n","  batch loop total:    116.72 s\n","\n","Epoch 004 | N_max=30 | LR=0.000100 | TrainLoss=0.1121\n","Epoch 004 completed in 257.34 s\n","  forward_one_play:   46.76 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.04 s\n","  backward+optim:      67.93 s\n","  batch loop total:    116.03 s\n","\n","Epoch 005 | N_max=30 | LR=0.000099 | TrainLoss=0.1022\n","Epoch 005 completed in 259.38 s\n","  forward_one_play:   46.90 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.04 s\n","  backward+optim:      68.84 s\n","  batch loop total:    117.09 s\n","\n","Epoch 006 | N_max=30 | LR=0.000098 | TrainLoss=0.0963\n","Epoch 006 completed in 256.56 s\n","  forward_one_play:   46.60 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.84 s\n","  batch loop total:    115.78 s\n","\n","Epoch 007 | N_max=30 | LR=0.000097 | TrainLoss=0.0898\n","Epoch 007 completed in 255.81 s\n","  forward_one_play:   46.73 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.79 s\n","  batch loop total:    115.87 s\n","\n","Epoch 008 | N_max=30 | LR=0.000096 | TrainLoss=0.0830\n","Epoch 008 completed in 254.54 s\n","  forward_one_play:   46.61 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.03 s\n","  backward+optim:      67.53 s\n","  batch loop total:    115.48 s\n","\n","Epoch 009 | N_max=30 | LR=0.000094 | TrainLoss=0.0759\n","Epoch 009 completed in 257.20 s\n","  forward_one_play:   46.95 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      67.88 s\n","  batch loop total:    116.19 s\n","\n","Epoch 010 | N_max=30 | LR=0.000091 | TrainLoss=0.0702\n","Epoch 010 completed in 259.22 s\n","  forward_one_play:   46.93 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      68.18 s\n","  batch loop total:    116.46 s\n","\n","Epoch 011 | N_max=30 | LR=0.000089 | TrainLoss=0.0659\n","Epoch 011 completed in 259.58 s\n","  forward_one_play:   46.96 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      68.42 s\n","  batch loop total:    116.73 s\n","\n","Epoch 012 | N_max=30 | LR=0.000086 | TrainLoss=0.0627\n","Epoch 012 completed in 257.77 s\n","  forward_one_play:   46.99 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.94 s\n","  batch loop total:    116.29 s\n","\n","Epoch 013 | N_max=30 | LR=0.000083 | TrainLoss=0.0602\n","Epoch 013 completed in 260.34 s\n","  forward_one_play:   47.31 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      68.54 s\n","  batch loop total:    117.22 s\n","\n","Epoch 014 | N_max=30 | LR=0.000080 | TrainLoss=0.0577\n","Epoch 014 completed in 262.22 s\n","  forward_one_play:   47.42 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      68.57 s\n","  batch loop total:    117.37 s\n","\n","Epoch 015 | N_max=30 | LR=0.000076 | TrainLoss=0.0555\n","Epoch 015 completed in 259.90 s\n","  forward_one_play:   47.09 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      68.36 s\n","  batch loop total:    116.81 s\n","\n","Epoch 016 | N_max=30 | LR=0.000073 | TrainLoss=0.0534\n","Epoch 016 completed in 265.09 s\n","  forward_one_play:   47.97 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.07 s\n","  backward+optim:      69.39 s\n","  batch loop total:    118.74 s\n","\n","Epoch 017 | N_max=30 | LR=0.000069 | TrainLoss=0.0518\n","Epoch 017 completed in 259.97 s\n","  forward_one_play:   47.10 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      68.47 s\n","  batch loop total:    116.92 s\n","\n","Epoch 018 | N_max=30 | LR=0.000065 | TrainLoss=0.0496\n","Epoch 018 completed in 258.62 s\n","  forward_one_play:   46.99 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      68.16 s\n","  batch loop total:    116.51 s\n","\n","Epoch 019 | N_max=30 | LR=0.000061 | TrainLoss=0.0479\n","Epoch 019 completed in 258.36 s\n","  forward_one_play:   46.92 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      68.06 s\n","  batch loop total:    116.34 s\n","\n","Epoch 020 | N_max=30 | LR=0.000056 | TrainLoss=0.0462\n","Epoch 020 completed in 256.24 s\n","  forward_one_play:   46.74 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.53 s\n","  batch loop total:    115.62 s\n","\n","Epoch 021 | N_max=30 | LR=0.000052 | TrainLoss=0.0450\n","Epoch 021 completed in 254.60 s\n","  forward_one_play:   46.46 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.04 s\n","  backward+optim:      67.30 s\n","  batch loop total:    115.10 s\n","\n","Epoch 022 | N_max=30 | LR=0.000048 | TrainLoss=0.0438\n","Epoch 022 completed in 256.48 s\n","  forward_one_play:   46.69 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.04 s\n","  backward+optim:      67.61 s\n","  batch loop total:    115.64 s\n","\n","Epoch 023 | N_max=30 | LR=0.000044 | TrainLoss=0.0430\n","Epoch 023 completed in 256.05 s\n","  forward_one_play:   46.82 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.62 s\n","  batch loop total:    115.80 s\n","\n","Epoch 024 | N_max=30 | LR=0.000039 | TrainLoss=0.0419\n","Epoch 024 completed in 254.51 s\n","  forward_one_play:   46.64 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.35 s\n","  batch loop total:    115.35 s\n","\n","Epoch 025 | N_max=30 | LR=0.000035 | TrainLoss=0.0411\n","Epoch 025 completed in 254.61 s\n","  forward_one_play:   46.79 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.31 s\n","  batch loop total:    115.46 s\n","\n","Epoch 026 | N_max=30 | LR=0.000031 | TrainLoss=0.0402\n","Epoch 026 completed in 255.75 s\n","  forward_one_play:   46.80 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.58 s\n","  batch loop total:    115.73 s\n","\n","Epoch 027 | N_max=30 | LR=0.000027 | TrainLoss=0.0397\n","Epoch 027 completed in 257.25 s\n","  forward_one_play:   46.95 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      68.21 s\n","  batch loop total:    116.53 s\n","\n","Epoch 028 | N_max=30 | LR=0.000024 | TrainLoss=0.0394\n","Epoch 028 completed in 255.49 s\n","  forward_one_play:   46.69 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.83 s\n","  batch loop total:    115.87 s\n","\n","Epoch 029 | N_max=30 | LR=0.000020 | TrainLoss=0.0386\n","Epoch 029 completed in 256.93 s\n","  forward_one_play:   46.89 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.04 s\n","  backward+optim:      67.91 s\n","  batch loop total:    116.14 s\n","\n","Epoch 030 | N_max=30 | LR=0.000017 | TrainLoss=0.0381\n","Epoch 030 completed in 255.86 s\n","  forward_one_play:   46.86 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.65 s\n","  batch loop total:    115.87 s\n","\n","Epoch 031 | N_max=30 | LR=0.000014 | TrainLoss=0.0375\n","Epoch 031 completed in 254.62 s\n","  forward_one_play:   46.67 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.04 s\n","  backward+optim:      67.48 s\n","  batch loop total:    115.49 s\n","\n","Epoch 032 | N_max=30 | LR=0.000011 | TrainLoss=0.0368\n","Epoch 032 completed in 254.03 s\n","  forward_one_play:   46.62 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.18 s\n","  batch loop total:    115.15 s\n","\n","Epoch 033 | N_max=30 | LR=0.000009 | TrainLoss=0.0368\n","Epoch 033 completed in 258.54 s\n","  forward_one_play:   46.91 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.70 s\n","  batch loop total:    115.97 s\n","\n","Epoch 034 | N_max=30 | LR=0.000006 | TrainLoss=0.0365\n","Epoch 034 completed in 256.95 s\n","  forward_one_play:   46.94 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      67.60 s\n","  batch loop total:    115.91 s\n","\n","Epoch 035 | N_max=30 | LR=0.000004 | TrainLoss=0.0362\n","Epoch 035 completed in 258.17 s\n","  forward_one_play:   47.14 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      68.05 s\n","  batch loop total:    116.55 s\n","\n","Epoch 036 | N_max=30 | LR=0.000003 | TrainLoss=0.0359\n","Epoch 036 completed in 257.72 s\n","  forward_one_play:   46.91 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.61 s\n","  batch loop total:    115.88 s\n","\n","Epoch 037 | N_max=30 | LR=0.000002 | TrainLoss=0.0358\n","Epoch 037 completed in 258.95 s\n","  forward_one_play:   46.88 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.86 s\n","  batch loop total:    116.10 s\n","\n","Epoch 038 | N_max=30 | LR=0.000001 | TrainLoss=0.0358\n","Epoch 038 completed in 255.38 s\n","  forward_one_play:   46.73 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.07 s\n","  batch loop total:    115.15 s\n","\n","Epoch 039 | N_max=30 | LR=0.000000 | TrainLoss=0.0357\n","Epoch 039 completed in 256.38 s\n","  forward_one_play:   46.81 s\n","  build_xy_true:      0.02 s\n","  loss compute:        1.05 s\n","  backward+optim:      67.38 s\n","  batch loop total:    115.55 s\n","\n","Epoch 040 | N_max=30 | LR=0.000000 | TrainLoss=0.0357\n","Epoch 040 completed in 260.44 s\n","  forward_one_play:   47.16 s\n","  build_xy_true:      0.03 s\n","  loss compute:        1.06 s\n","  backward+optim:      68.28 s\n","  batch loop total:    116.80 s\n","\n","üìä Dataset: 12966 plays for training (no validation split)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipython-input-1028069646.py\", line 310, in <cell line: 0>\n","    model, _ = train_loop_full_data(\n","               ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-1028069646.py\", line 263, in train_loop_full_data\n","    xy_pred, mask = model.forward_batch(batch, N_max_curr=N_curr)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-1028069646.py\", line 190, in forward_batch\n","    h_nodes = self.encoder(x_hist)            # (P,D)\n","              ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-2539799799.py\", line 120, in forward\n","    out = self.encoder(x)       # (P,K,D)\n","          ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 524, in forward\n","    output = mod(\n","             ^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 928, in forward\n","    x = x + self._sa_block(\n","            ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 949, in _sa_block\n","    x = self.self_attn(\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\", line 1488, in forward\n","    attn_output, attn_output_weights = F.multi_head_attention_forward(\n","                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 6487, in multi_head_attention_forward\n","    attn_output = scaled_dot_product_attention(\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/inspect.py\", line 1697, in getframeinfo\n","    positions = _get_code_position_from_tb(frame)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/inspect.py\", line 1679, in _get_code_position_from_tb\n","    return _get_code_position(code, instruction_index)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/inspect.py\", line 1686, in _get_code_position\n","    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1028069646.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     model, _ = train_loop_full_data(\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mdf_in_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_out_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphs_fast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1028069646.py\u001b[0m in \u001b[0;36mtrain_loop_full_data\u001b[0;34m(df_in_norm, df_out_norm, graphs_fast, K, N_max, batch_size, num_workers, lr, wd, warmup_epochs, total_epochs, pretrain_epochs)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mxy_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_max_curr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mt_forward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1028069646.py\u001b[0m in \u001b[0;36mforward_batch\u001b[0;34m(self, batch, N_max_curr)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mh_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hist\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# (P,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mh_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2539799799.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# (P,K,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m             x = x + self._sa_block(\n\u001b[0m\u001b[1;32m    929\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    948\u001b[0m     ) -> Tensor:\n\u001b[0;32m--> 949\u001b[0;31m         x = self.self_attn(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1489\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6487\u001b[0;31m         attn_output = scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m   6488\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b22N-GHqG4Dn","executionInfo":{"status":"aborted","timestamp":1763942181610,"user_tz":300,"elapsed":12,"user":{"displayName":"Swapnita Sahu","userId":"05446195139935344550"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["6scEdDyWLlMD","-cQfTHBtSFwd","rwke3RGHSmjy","nPPXU07KSJmA","xVh8ub0rUFMP"],"provenance":[{"file_id":"1dezlte657tcEjH69JV_UYqjpcpQosC67","timestamp":1763928693738},{"file_id":"1Ce1gpt_rjtdIcxQRtHZWB9Yw343eBtAX","timestamp":1763877275108},{"file_id":"15FeuH6gBkGJ57dM5wQDhRU737ppfWWH7","timestamp":1763577568559},{"file_id":"1G6aFWiWdbDMFXbm5sSN2WhC8skslz4BD","timestamp":1763402033208},{"file_id":"1QDGwKy4cFEu7wk12w1oE9Gv3_ptbIr9m","timestamp":1762728979656}],"authorship_tag":"ABX9TyMiI2srCXzIqqFGeTKclvie"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}